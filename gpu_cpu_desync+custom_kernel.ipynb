{
  "metadata": {
    "kernelspec": {
      "name": "xpython",
      "display_name": "Python 3.13 (XPython)",
      "language": "python"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "cb1c15fb-2f83-459d-94ea-dcccbcbf4a58",
      "cell_type": "markdown",
      "source": "Hi. Here are a few things I have learnt and found out about gpu optimization for ML training.\n\nOne of the main goals of gpu optimizatin is to maximize its utilization rate, since both buying gpu and keeping the powers on cost a lot of money. Below is a simple demo training script.",
      "metadata": {}
    },
    {
      "id": "14fe869f-7e2b-49ea-ab71-27f01150d4cc",
      "cell_type": "code",
      "source": "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nX, y = torch.randn(10000, 64), torch.randn(10000, 1)\nloader = DataLoader(TensorDataset(X, y), batch_size=256, shuffle=True)\n\nmodel = nn.Linear(64, 1).to(device)\nopt = torch.optim.SGD(model.parameters(), 0.01)\nloss_fn = nn.MSELoss()\n\nrunning_loss, running_count = 0.0, 0\n\nfor step, (xb, yb) in enumerate(loader, 1):\n    xb, yb = xb.to(device), yb.to(device)\n    loss = loss_fn(model(xb), yb)\n    if torch.isnan(loss).any().item():\n        print(f\"NaN at step {step}\"); break\n    opt.zero_grad(); loss.backward(); opt.step()\n    running_loss += loss.item() * xb.size(0)\n    running_count += xb.size(0)\n\n    if step % 50 == 0:\n        print(f\"avg_loss: {running_loss / running_count:.4f}\")\n        running_loss, running_count = 0.0, 0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e7b11fa8-0bf9-4ac9-a542-6e1a11d1cca8",
      "cell_type": "markdown",
      "source": "Following are several ways this script can be optimized for gpu usage, mainly through cpu & gpu desyncronization.",
      "metadata": {}
    },
    {
      "id": "84177b67-5d0f-4099-bcea-b39935baef4c",
      "cell_type": "code",
      "source": "for step, (xb, yb) in enumerate(loader, 1):\n    xb, yb = xb.to(device), yb.to(device)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8c9b8c7e-20a6-4078-a907-d2308442f90b",
      "cell_type": "markdown",
      "source": "Right now, every time gpu finishes processing a training point, it has to wait for the for-loop running on the cpu to push the next data point to its memory. A better way to do this is:",
      "metadata": {}
    },
    {
      "id": "59325e3c-e3db-43e0-9115-6da839824a2b",
      "cell_type": "code",
      "source": "loader = DataLoader(TensorDataset(X, y), batch_size=256, shuffle=True, pin_memory=True)\nfor step, (xb, yb) in enumerate(loader, 1):\n    xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "pWhen the cpu loads the data into its memory, it pins them into several reserved physical addresses, so they won't move around due to other tasks running on the cpu. This allows the gpu to copy the data asynchronously by simply knowing the relevant physical memory addresses. non-blocking is set to True so that gpu can access the data itself without waiting for cpu's explicit allocation.",
      "id": "93cd553f-ca42-4c77-8912-f10b9c9d657f"
    },
    {
      "id": "3492c858-c042-4702-946c-483eccd94795",
      "cell_type": "markdown",
      "source": "if torch.isnan(loss).any().item():\n    print(f\"NaN at step {step}\"); break",
      "metadata": {}
    },
    {
      "id": "baafbde1-f640-4fe6-b443-5e6d8f5e6743",
      "cell_type": "markdown",
      "source": "The if-statement requires cpu execution, meaning that the gpu must send the loss value it has computed back to the cpu to check for NaN, causing a break in the gpu's training loop. This could be avoided by:",
      "metadata": {}
    },
    {
      "id": "43d507dc-2d46-46d0-93d8-55b85bed8537",
      "cell_type": "code",
      "source": [
        "nan_flag = torch.zeros(1, dtype=torch.bool, device=device)\n",
        "nan_flag |= torch.isnan(loss).any()\n",
        "if step % 50 == 0:\n",
        "    if nan_flag.item(): print(f\"NaN at step {step}\"); break"
      ],
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a66da5b-bad4-4116-9a54-19c0edf8c0d9",
      "cell_type": "markdown",
      "source": [
        "Here, we initiate nan_flag as a 1 byte bool tensor set to 0 on the gpu. torch.isnan(loss).any() checks on the gpu if there is any NaN tensor and create a bool tensor. We can accumulate the result on the gpu by applying OR between the nan_flag and each bool tensor s.t. nan_flag will be set to 1/True if at least 1 NaN is dected. Then, we can check every 50 steps on the cpu if nan_flag is True.\n",
        "\n",
        "The potential concern is the overhead from running all the extra training loops after a NaN is detected. Turns out the time saved from not having to copy between gpu's and cpu's memories is far greater than what it takes to complete a few extra loops."
      ],
      "metadata": {}
    },
    {
      "id": "617b352f-7f4f-4c3b-b310-89ae815e452a",
      "cell_type": "code",
      "source": "running_loss += loss.item() * xb.size(0)\nrunning_count += xb.size(0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "64e07f68-817a-4b0a-81ae-f97eff2b6016",
      "cell_type": "markdown",
      "source": "Now it's cpu's turn to wait for the gpu to finish computing the losses. Although cpu idle time does not matter as much as gpu idle time, it would actually later result in gpu having to wait for the cpu to finish the computes here. Therefore, the best way to do this is to accumulate the loss metrics on the gpu.",
      "metadata": {}
    },
    {
      "id": "83f5f2bc-06b9-42de-b4d5-e57e80944549",
      "cell_type": "code",
      "source": "loss_sum = torch.zeros(1, device=device)\ncount = torch.zeros(1, device=device)\nloss_sum += loss.detach() * xb.size(0); \ncount += xb.size(0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "45162b80-21d6-4240-aed1-68111914e155",
      "cell_type": "markdown",
      "source": "Similar to the nan_flag, here we initiate the loss and count vars as gpu tensors. Careful that loss should be detached if computed on gpu to to remove its computation graph from the memory to save some spaces.",
      "metadata": {}
    },
    {
      "id": "1d7396f8-8ae5-4d36-b00e-c79b23b94185",
      "cell_type": "markdown",
      "source": "Here is the complete script after the optimization. The key take away is to eliminate any bubble created from cpu & gpu attemps to syncronize through desyncronization. CUDA profilling tools like Nvidia Nsight is very useful for identifying the specific areas of code that can be optimized in this regard. ",
      "metadata": {}
    },
    {
      "id": "622d84af-780a-4c3d-aa7e-f919f0036dfc",
      "cell_type": "code",
      "source": "import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nX, y = torch.randn(10000, 64), torch.randn(10000, 1)\nloader = DataLoader(TensorDataset(X, y), batch_size=256, shuffle=True, pin_memory=True)\n\nmodel = nn.Linear(64, 1).to(device)\nopt = torch.optim.SGD(model.parameters(), 0.01)\nloss_fn = nn.MSELoss()\n\nloss_sum = torch.zeros(1, device=device)\ncount    = torch.zeros(1, device=device)\nnan_flag = torch.zeros(1, dtype=torch.uint8, device=device)\n\nfor step, (xb, yb) in enumerate(loader, 1):\n    xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n    loss = loss_fn(model(xb), yb)\n    nan_flag |= torch.isnan(loss).any()\n    opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n    loss_sum += loss.detach() * xb.size(0); count += xb.size(0)\n\n    if step % 50 == 0:\n        if nan_flag.item(): print(f\"NaN at step {step}\"); break\n        print(f\"avg_loss: {(loss_sum/count).cpu().item():.4f}\")\n        loss_sum.zero_(); count.zero_()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ac466d13-6fd4-4b04-b1f6-248021f72c11",
      "cell_type": "markdown",
      "source": "The gpu itself also comes with many bottlenecks in areas such as compute, memory, and kernel scheduling. A kernel is a singular, containerized function that runs on a gpu with its allocated cores and memory. As such, having too many kernels will consumer a significant amount of memories. This can be avoided via kernel fusion either through torch.compile(), which is not optimized in many cases, or writting custom CUDA kernel in C++.",
      "metadata": {}
    },
    {
      "id": "a5c01d22-9b3e-4297-a3ec-6dd9f64fa594",
      "cell_type": "markdown",
      "source": "Below is my first attempt at writting a custom kernel, which add two arrays of floats, a common step for adding the bias vectors in a linear layer.",
      "metadata": {}
    },
    {
      "id": "35f0c358-5e38-4bf9-b7e1-63634484a414",
      "cell_type": "code",
      "source": "#include <stdio.h>\n#include <cuda_runtime.h>\n\n// Kernel: C[i] = A[i] + B[i]\n__global__ void vec_add(const float* A, const float* B, float* C, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        C[idx] = A[idx] + B[idx];\n    }\n}\n\nint main() {\n    int N = 10;\n    size_t bytes = N * sizeof(float);\n\n    // Host arrays\n    float hA[10], hB[10], hC[10];\n    for (int i = 0; i < N; i++) { hA[i] = i; hB[i] = i * 2; }\n\n    // Device arrays\n    float *dA, *dB, *dC;\n    cudaMalloc(&dA, bytes);\n    cudaMalloc(&dB, bytes);\n    cudaMalloc(&dC, bytes);\n\n    cudaMemcpy(dA, hA, bytes, cudaMemcpyHostToDevice);\n    cudaMemcpy(dB, hB, bytes, cudaMemcpyHostToDevice);\n\n    // Launch with 1 block of N threads\n    vec_add<<<1, N>>>(dA, dB, dC, N);\n    cudaDeviceSynchronize();\n\n    cudaMemcpy(hC, dC, bytes, cudaMemcpyDeviceToHost);\n\n    // Print result\n    for (int i = 0; i < N; i++) {\n        printf(\"%f + %f = %f\\n\", hA[i], hB[i], hC[i]);\n    }\n\n    cudaFree(dA); cudaFree(dB); cudaFree(dC);\n    return 0;\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1f29914f-1911-4568-9e7f-baa1294fcb03",
      "cell_type": "markdown",
      "source": "CUDA also has a great feature called CUDA graph, which significanly reduces the overhead from kernel scheduling by dynamically generating modular, reusable template for instant scheduling replay. I will check it out later. Here is the link for anyone interested https://developer.nvidia.com/blog/cuda-graphs/.",
      "metadata": {}
    }
  ]
}